{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Tutorial: Análise de Sentimentos em Português com BERT\n",
    "\n",
    "Neste tutorial, vamos aprender como usar o modelo BERT para prever o **sentimento (positivo ou negativo)** de comentários em português no IMDB.\n",
    "\n",
    "Utilizaremos o modelo **BERT pré-treinado da Hugging Face** (`neuralmind/bert-base-portuguese-cased`) e faremos o fine-tuning com PyTorch.\n",
    "\n",
    "## 📌 1. O que é o BERT?\n",
    "\n",
    "O **BERT** (Bidirectional Encoder Representations from Transformers) é um modelo baseado em Transformers, ideal para tarefas de NLP como classificação de texto, análise de sentimentos, entre outros.\n",
    "\n",
    "Neste caso, usamos o **encoder** do BERT para classificar frases como **positivas** ou **negativas**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.env/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in ./.env/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in ./.env/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in ./.env/lib/python3.11/site-packages (4.53.0)\n",
      "Requirement already satisfied: datasets in ./.env/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.env/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: plotly in ./.env/lib/python3.11/site-packages (6.2.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.env/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.env/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.env/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.env/lib/python3.11/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.env/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.env/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.env/lib/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.env/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.env/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.env/lib/python3.11/site-packages (from plotly) (1.45.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.env/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.11/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio \\\n",
    "    transformers datasets scikit-learn pandas plotly \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in ./.env/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: ipywidgets in ./.env/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: notebook in ./.env/lib/python3.11/site-packages (from jupyter) (7.4.4)\n",
      "Requirement already satisfied: jupyter-console in ./.env/lib/python3.11/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in ./.env/lib/python3.11/site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in ./.env/lib/python3.11/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyterlab in ./.env/lib/python3.11/site-packages (from jupyter) (4.4.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.env/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.env/lib/python3.11/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.env/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.env/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.env/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./.env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: appnope in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (27.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.env/lib/python3.11/site-packages (from ipykernel->jupyter) (6.5.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./.env/lib/python3.11/site-packages (from jupyterlab->jupyter) (65.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./.env/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./.env/lib/python3.11/site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in ./.env/lib/python3.11/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./.env/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: anyio in ./.env/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.env/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./.env/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.env/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.env/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.env/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.8)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.22.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./.env/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./.env/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./.env/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./.env/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.24.0)\n",
      "Requirement already satisfied: requests>=2.31 in ./.env/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.env/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.env/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.env/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.env/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.env/lib/python3.11/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./.env/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.env/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.env/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.env/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.env/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./.env/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in ./.env/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in ./.env/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./.env/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.env/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
      "Requirement already satisfied: fqdn in ./.env/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./.env/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./.env/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: uri-template in ./.env/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./.env/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./.env/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.env/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./.env/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./.env/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20250516)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42ba2473ab4439693c53fddd35baab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📂 2. Carregando os Dados\n",
    "\n",
    "Vamos usar um dataset com resenhas do IMDB traduzidas para português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_en  \\\n",
       "id                                                      \n",
       "1   Once again Mr. Costner has dragged out a movie...   \n",
       "2   This is an example of why the majority of acti...   \n",
       "3   First of all I hate those moronic rappers, who...   \n",
       "4   Not even the Beatles could write songs everyon...   \n",
       "5   Brass pictures movies is not a fitting word fo...   \n",
       "\n",
       "                                              text_pt sentiment  \n",
       "id                                                               \n",
       "1   Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "2   Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "3   Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "4   Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "5   Filmes de fotos de latão não é uma palavra apr...       neg  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carrega o dataset\n",
    "data = pd.read_csv(\"dataset/imdb-reviews-pt-br.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Estrutura do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49459 entries, 1 to 49460\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text_en    49459 non-null  object\n",
      " 1   text_pt    49459 non-null  object\n",
      " 2   sentiment  49459 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos 3 colunas:\n",
    "\n",
    "* `text_en`: texto original em inglês\n",
    "* `text_pt`: texto traduzido para português\n",
    "* `sentiment`: rótulo de sentimento (`pos` ou `neg`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 3. Explorando a Distribuição de Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts().plot.pie(autopct=\"%.2f\", explode=[0.01, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset está **balanceado**, então podemos usar **acurácia** como métrica principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 4. Dividindo os Dados\n",
    "\n",
    "Vamos criar conjuntos de treino, validação (dev) e teste na proporção 90/5/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_dev_size = int(0.05 * len(data))\n",
    "train_dev, test = train_test_split(\n",
    "    data, test_size=test_dev_size, stratify=data[\"sentiment\"], random_state=42\n",
    ")\n",
    "train, dev = train_test_split(\n",
    "    train_dev, test_size=test_dev_size, stratify=train_dev[\"sentiment\"], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔁 5. Preparando o Dataset para o BERT\n",
    "\n",
    "Criamos uma classe para tokenizar e estruturar os dados conforme esperado pelo BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ImdbPt(Dataset):\n",
    "    def __init__(self, tokenizer, X, y):\n",
    "        tokenized = tokenizer(list(X), truncation=True, max_length=512)\n",
    "        self.samples = [\n",
    "            {**{k: tokenized[k][i] for k in tokenized}, \"labels\": y.iloc[i]}\n",
    "            for i in range(len(X))\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧼 6. Tokenizando e Criando Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      4\u001b[39m tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mneuralmind/bert-base-portuguese-cased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_dataset = \u001b[43mImdbPt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m dev_dataset = ImdbPt(tokenizer, dev[\u001b[33m\"\u001b[39m\u001b[33mtext_pt\u001b[39m\u001b[33m\"\u001b[39m], (dev[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m))\n\u001b[32m     10\u001b[39m test_dataset = ImdbPt(\n\u001b[32m     11\u001b[39m     tokenizer, test[\u001b[33m\"\u001b[39m\u001b[33mtext_pt\u001b[39m\u001b[33m\"\u001b[39m], (test[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mImdbPt.__init__\u001b[39m\u001b[34m(self, tokenizer, X, y)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, X, y):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tokenized = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.samples = [\n\u001b[32m      8\u001b[39m         {**{k: tokenized[k][i] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tokenized}, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: y.iloc[i]}\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))\n\u001b[32m     10\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2855\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2854\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2943\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2938\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2939\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2940\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2941\u001b[39m         )\n\u001b[32m   2942\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2962\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2963\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2966\u001b[39m         text=text,\n\u001b[32m   2967\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2985\u001b[39m         **kwargs,\n\u001b[32m   2986\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3144\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3134\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3135\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3136\u001b[39m     padding=padding,\n\u001b[32m   3137\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3141\u001b[39m     **kwargs,\n\u001b[32m   3142\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3146\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3164\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils.py:887\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    885\u001b[39m     ids, pair_ids = ids_or_pair_ids\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m second_ids = get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    889\u001b[39m input_ids.append((first_ids, second_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils.py:854\u001b[39m, in \u001b[36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    855\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/transformers/models/bert/tokenization_bert.py:168\u001b[39m, in \u001b[36mBertTokenizer._tokenize\u001b[39m\u001b[34m(self, text, split_special_tokens)\u001b[39m\n\u001b[32m    166\u001b[39m             split_tokens.append(token)\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m             split_tokens += \u001b[38;5;28mself\u001b[39m.wordpiece_tokenizer.tokenize(token)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     split_tokens = \u001b[38;5;28mself\u001b[39m.wordpiece_tokenizer.tokenize(text)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "train_dataset = ImdbPt(\n",
    "    tokenizer, train[\"text_pt\"], (train[\"sentiment\"] == \"pos\").astype(int)\n",
    ")\n",
    "dev_dataset = ImdbPt(tokenizer, dev[\"text_pt\"], (dev[\"sentiment\"] == \"pos\").astype(int))\n",
    "test_dataset = ImdbPt(\n",
    "    tokenizer, test[\"text_pt\"], (test[\"sentiment\"] == \"pos\").astype(int)\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=collator)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=2, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ 7. Carregando o Modelo e Configurando o Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Congelando o BERT no início\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 8. Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_inputs_to_device(inputs, device):\n",
    "    return {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    tp = tn = fp = fn = 0\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = send_inputs_to_device(batch, device)\n",
    "            loss, scores = model(**batch)[:2]\n",
    "            losses.append(loss.item())\n",
    "            pred = scores.argmax(dim=1)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            tp += ((pred == 1) & (labels == 1)).sum().item()\n",
    "            tn += ((pred == 0) & (labels == 0)).sum().item()\n",
    "            fp += ((pred == 1) & (labels == 0)).sum().item()\n",
    "            fn += ((pred == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(\n",
    "        f\"Dev loss: {sum(losses)/len(losses):.2f}; Acc: {acc:.2f}; tp: {tp}; tn: {tn}; fp: {fp}; fn: {fn}\"\n",
    "    )\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 9. Treinando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Dentro do loop de treino ou validação:\n",
    "# del inputs, loss, logits\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()  # limpa cache no MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548e3f865f0a46c99d30f143122fcef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d397b73c560489a85138f36830342d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/22258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 0.73; Acc: 0.45; tp: 243; tn: 872; fp: 366; fn: 991\n",
      "Dev loss: 0.72; Acc: 0.45; tp: 257; tn: 862; fp: 376; fn: 977\n",
      "Dev loss: 0.72; Acc: 0.45; tp: 262; tn: 855; fp: 383; fn: 972\n",
      "Dev loss: 0.71; Acc: 0.45; tp: 277; tn: 832; fp: 406; fn: 957\n",
      "Dev loss: 0.71; Acc: 0.45; tp: 296; tn: 818; fp: 420; fn: 938\n",
      "Dev loss: 0.64; Acc: 0.59; tp: 1214; tn: 234; fp: 1004; fn: 20\n",
      "Dev loss: 0.41; Acc: 0.82; tp: 1195; tn: 835; fp: 403; fn: 39\n",
      "Dev loss: 0.28; Acc: 0.89; tp: 1123; tn: 1078; fp: 160; fn: 111\n",
      "Dev loss: 0.34; Acc: 0.86; tp: 1180; tn: 950; fp: 288; fn: 54\n",
      "Dev loss: 0.27; Acc: 0.90; tp: 1114; tn: 1104; fp: 134; fn: 120\n",
      "Dev loss: 0.25; Acc: 0.90; tp: 1102; tn: 1128; fp: 110; fn: 132\n",
      "Dev loss: 0.29; Acc: 0.89; tp: 1167; tn: 1027; fp: 211; fn: 67\n",
      "Dev loss: 0.26; Acc: 0.90; tp: 1128; tn: 1102; fp: 136; fn: 106\n",
      "Dev loss: 0.25; Acc: 0.90; tp: 1113; tn: 1123; fp: 115; fn: 121\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.48 GB, other allocations: 32.74 GB, max allowed: 36.27 GB). Tried to allocate 87.29 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m loss, _ = model(**batch)[:\u001b[32m2\u001b[39m]\n\u001b[32m     16\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m scheduler.step()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Loss moving average\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/redhat/ai/chatbot_sentiment_analysis/.env/lib/python3.11/site-packages/torch/optim/adam.py:525\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    523\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m         denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / bias_correction2_sqrt).add_(eps)\n\u001b[32m    527\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 3.48 GB, other allocations: 32.74 GB, max allowed: 36.27 GB). Tried to allocate 87.29 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loss_acc = 0\n",
    "alpha = 0.95\n",
    "for epoch in tqdm(range(1)):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "    ):\n",
    "        if epoch * len(train_loader) + step == 800:\n",
    "            for param in model.base_model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        batch = send_inputs_to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(**batch)[:2]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Loss moving average\n",
    "        loss_acc = (\n",
    "            loss.item() if step == 0 else loss_acc * alpha + loss.item() * (1 - alpha)\n",
    "        )\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            evaluate(model, dev_loader, device)\n",
    "\n",
    "    model.save_pretrained(f\"/models/working/checkpoints/epoch{epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📉 10. Curva ROC e Threshold Ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import plotly.express as px\n",
    "\n",
    "model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        batch = send_inputs_to_device(batch, device)\n",
    "        _, scores = model(**batch)[:2]\n",
    "        probs = F.softmax(scores, dim=1)[:, 1]\n",
    "        preds.append(probs.cpu())\n",
    "        true_labels.append(batch[\"labels\"].cpu())\n",
    "\n",
    "preds = torch.cat(preds).numpy()\n",
    "true_labels = torch.cat(true_labels).numpy()\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_labels, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "px.scatter(x=fpr, y=tpr, color=thresholds, title=\"Curva ROC\").show()\n",
    "\n",
    "accuracies = [metrics.accuracy_score(preds > th, true_labels) for th in thresholds]\n",
    "px.scatter(x=thresholds, y=accuracies, title=\"Acurácia por Threshold\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧾 11. Avaliação Final no Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = send_inputs_to_device(batch, device)\n",
    "        _, scores = model(**batch)[:2]\n",
    "        probs = F.softmax(scores, dim=1)[:, 1]\n",
    "        preds.append(probs.cpu())\n",
    "        true_labels.append(batch[\"labels\"].cpu())\n",
    "\n",
    "preds = torch.cat(preds).numpy()\n",
    "true_labels = torch.cat(true_labels).numpy()\n",
    "\n",
    "# Usando o melhor threshold encontrado\n",
    "final_acc = metrics.accuracy_score(preds > 0.67, true_labels)\n",
    "print(f\"Acurácia no Teste: {final_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Conclusão\n",
    "\n",
    "Neste tutorial, mostramos como aplicar o BERT para análise de sentimentos em português:\n",
    "\n",
    "* Usamos o modelo `neuralmind/bert-base-portuguese-cased`\n",
    "* Processamos e tokenizamos os dados\n",
    "* Treinamos o modelo com congelamento inicial\n",
    "* Avaliamos e otimizamos o threshold via curva ROC\n",
    "\n",
    "Essa abordagem pode ser adaptada para outras tarefas de NLP em português. Experimente com datasets diferentes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
